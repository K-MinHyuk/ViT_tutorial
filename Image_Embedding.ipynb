{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embedding\n",
    "\n",
    "![](./img/model_architecture.png)\n",
    "\n",
    "<figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    "    \"ViT architecture\"\n",
    "  </figcaption>\n",
    "\n",
    "가장 먼저 만나게 되는 Image Embedding 파트 입니다!\n",
    "\n",
    "다음 4단계로 Image 를 Embedding 합니다.\n",
    "- Image 를 Patch 라는 단위로 나누기\n",
    "- Image 데이터를 Patch 를 기준으로 Flatten 하기\n",
    "- Flatten 된 Vector 에 Class token 을 추가하기\n",
    "- Class token 이 추가된 Vector 에 Positional Embedding 더하기 \n",
    "\n",
    "> 추가적으로, Einops package 에 대한 내용을 같이 작성하려고 하다가, 너무 길어져서 따로 빼 두었습니다.\\\n",
    "> 이 내용은 `einops_toturial_for_vit.ipynb` 에서 확인할 수 있습니다.\\\n",
    "> 부가적인 부분이지만 내용의 흐름상 꼭 참고해주시면 좋겠습니다:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([6, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ims = torch.Tensor(np.load('./resources/test_images.npy', allow_pickle=False))\n",
    "ims = rearrange(ims, 'b h w c -> b c h w')\n",
    "print(type(ims), ims.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 차원은, `matplotlib` 에 친화적인 `b h w c` 가 아닌\n",
    "\n",
    "`pytorch` 친화적인 `b c h w` 로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ims : torch.Size([6, 3, 96, 96])\n",
      "patches : torch.Size([6, 36, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16 # 16 pixels\n",
    "\n",
    "print('ims :', ims.shape)\n",
    "patches = rearrange(ims, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
