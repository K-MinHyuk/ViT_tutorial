{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 구조의 이해\n",
    "<center>\n",
    "\n",
    "![](./img/transformer.png)\n",
    "<figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    "    \"fig 1: Transformer Model\"\n",
    "</figcaption>\n",
    "  \n",
    "</center>\n",
    "\n",
    "위 fig 1 은, Attention is all you need 논문에서 제안된 transformer 모델의 architecture 입니다.\\\n",
    "encoder & decoder 구조로, 많은 분야에 큰 영향을 주었죠.\n",
    "\n",
    "특히, transformer 의 꽃이라고 말 할 수 있는 부분은 Attention 구조입니다.\\\n",
    "해당 구조를 통해서 많은 성능 개선과 아이디어들이 등장할 수 있게 되었죠. \n",
    "\n",
    "다양한 논문들이 Transformer 를 변형하여 새로운 task 를 다루게 되었습니다.\\\n",
    "대표적으로 다음과 같은 변형 방법이 있습니다. \n",
    "\n",
    "- Transformer 의 encoder 만을 사용하는 방법 \n",
    "- Transformer 의 decoder 만을 사용하는 방법\n",
    "\n",
    "Transformer 의 encoder 만을 사용하는 방법으로 대표적인 예시는 BERT (bidirectional Encoder Representations from Transformers) 가 있겠네요.\\\n",
    "또한, **ViT 도 encoder 만을 사용하는 방법**입니다.\n",
    "\n",
    "Transformer 의 decoder 만을 사용하는 방법으로 대표적인 예시는 GPT 입니다. \n",
    "\n",
    "\n",
    "><details><summary>\n",
    ">\n",
    ">### BERT & GPT\n",
    "></summary>\n",
    "> BERT 와 GPT 는 모두 Transformer 의 일부 형태를 사용합니다.\n",
    ">\n",
    "> 다만, 모델의 구조만을 보면 서로 동일하다고 생각하기 쉽습니다.\n",
    ">\n",
    "> 이 두 모델을 정확하게 이해하려면, 각 모델이 **학습하는 과정**과 **inference 과정**을 비교해 보는 것이 좋습니다. \n",
    "><center>\n",
    "> \n",
    "> ![](./img/BertAndGpt.png)\n",
    "> <figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    ">   fig 2: BERT & GPT \n",
    ">    출처: Book [Foundation Models for Natural Language Processing Pre-trained Language Models Integrating Media]\n",
    ">   </figcaption>\n",
    ">   \n",
    "> </center>\n",
    ">\n",
    "> #### BERT\n",
    "> BERT 는 전체 시퀀스를 한번에 입력 받습니다. \n",
    "> 다만, 특정 문장을 [MASK] 처리하여 해당 [MASK] 에 들어갈 문장을 학습하고, 추론합니다. \n",
    ">\n",
    "> #### GPT\n",
    "> GPT 는 앞선 문장들을 입력으로 받으며, 다음에 등장할 단어를 추론하는 방식으로 학습합니다. \n",
    "> 그러므로, [the mouse eats cheese] 라는 문장을 아래와 같이 여러번의 입력으로 나누고 recursive 하게 다음 단어를 추론하게 됩니다.\n",
    "> - the\n",
    "> - the mouse\n",
    "> - the mouse eats \n",
    "> - ...\n",
    "> \n",
    "></details>\n",
    "\n",
    "\n",
    "이전 장에서 ViT 의 입력으로 사용되는 이미지를 patch 단위로 나누었고, vector 로 embedding 했습니다. \n",
    "\n",
    "이제, Transformer 의 encoder 를 구현하기 위한 Multi-Head Attention 을 알아보죠!\n",
    "\n",
    "----------\n",
    "# Multi-Head Attention\n",
    "<center>\n",
    "\n",
    "![](./img/multi_head_attention.png)\n",
    "<figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    "    \"fig 3: multi-head-attention\"\n",
    "</figcaption>\n",
    "  \n",
    "</center>\n",
    "\n",
    "MHA는 위 그림과 같이 진행됩니다. VIT에서의 MHA는 QKV가 모두 동일한 embedding vector 입니다.\\\n",
    "입력텐서는 head 개의 Linear Projection 을 통해 임베딩된 후,각각 Scaled Dot-Product Attention을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "from utils.vit_utils import Image_Embedding # 이전 장의 image embedding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([6, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ims = torch.Tensor(np.load('./resources/test_images.npy', allow_pickle=False))\n",
    "ims = rearrange(ims, 'b h w c -> b c h w')\n",
    "print(type(ims), ims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1            [-1, 3, 96, 96]          55,296\n",
      "            Conv2d-2            [-1, 768, 6, 6]         590,592\n",
      "         LayerNorm-3            [-1, 768, 6, 6]          55,296\n",
      "================================================================\n",
      "Total params: 701,184\n",
      "Trainable params: 701,184\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.63\n",
      "Params size (MB): 2.67\n",
      "Estimated Total Size (MB): 3.41\n",
      "----------------------------------------------------------------\n",
      "Output shape: torch.Size([6, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "image_embedding = Image_Embedding(image_size = ims.shape, patch_size=16).to(device)\n",
    "embedded_tensor = image_embedding(ims.to(device))\n",
    "summary(image_embedding, ims.shape[1:])\n",
    "print('Output shape: {}'.format(embedded_tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 6장의 이미지는 [6, 37, 768]크기로 embedding 된 것을 알 수 있습니다.\\\n",
    "이제 이 tensor 를 가지고 Multi Head Attention 을 구현해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys shape    : torch.Size([6, 8, 37, 96])\n",
      "queries shape : torch.Size([6, 8, 37, 96])\n",
      "values        : torch.Size([6, 8, 37, 96])\n"
     ]
    }
   ],
   "source": [
    "embedding_size = embedded_tensor.shape[-1]\n",
    "num_heads = 8\n",
    "\n",
    "K = nn.Linear(embedding_size, embedding_size).to(device)\n",
    "Q = nn.Linear(embedding_size, embedding_size).to(device)\n",
    "V = nn.Linear(embedding_size, embedding_size).to(device)\n",
    "\n",
    "keys    = rearrange(K(embedded_tensor), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "queries = rearrange(Q(embedded_tensor), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values  = rearrange(V(embedded_tensor), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print('''keys shape    : {}\n",
    "queries shape : {}\n",
    "values        : {}'''.format(keys.shape, queries.shape, values.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 차원을 다시 한 번 점검해 보도록 합시다. \n",
    "```python\n",
    "torch.Size([    6,    8,    37,             96])\n",
    "torch.Size([Batch, Head, Patch, Embedding size])\n",
    "```\n",
    "\n",
    "이제는 Scaled dot-product attention 을 구현해 봅시다.\n",
    "\n",
    "-------\n",
    "\n",
    "## Scaled dot-product attention\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "    <img src=\"./img/Scaled_dot_product_attention.png\" alt=\"Scaled dot-product attention\" width=\"50%\" height=\"50%\">\n",
    "</figure>\n",
    "<figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    "    \"fig 3: Scaled dot-product attention\"\n",
    "</figcaption>\n",
    "\n",
    "</center>\n",
    "\n",
    "위 그림을 수식화 하면 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "코드로 구현해 보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK : torch.Size([6, 8, 37, 37])\n"
     ]
    }
   ],
   "source": [
    "# QK^T\n",
    "QK = torch.einsum('b h q d, b h k d -> b h q k', queries, keys)\n",
    "print('QK :', QK.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einops 의 사용 방법에 어느 정도 익숙해 졌으리라 믿습니다.\n",
    "\n",
    "그러므로 위 문법도 내적의 관점에서 잘 이해해 보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Attention Score : torch.Size([6, 8, 37, 37])\n"
     ]
    }
   ],
   "source": [
    "# Attention Score\n",
    "scaling = embedding_size ** (1/2)\n",
    "attention_score = F.softmax(QK/scaling, dim=-1)\n",
    "print('Shape of Attention Score :', attention_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의, Attention Score 의 차원에 대해서 다시 정리해 보죠.\n",
    "\n",
    "```python\n",
    "torch.Size([    6,    8,    37,             37])\n",
    "torch.Size([Batch, Head, Patch, Patch])\n",
    "```\n",
    "\n",
    "즉, 각 Patch 들 끼리의 Attention Score 라는 것을 잊지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Representation : torch.Size([6, 8, 37, 96])\n"
     ]
    }
   ],
   "source": [
    "# Attention Score * values\n",
    "representation = torch.einsum('b h p d, b h d v -> b h p v ', attention_score, values)\n",
    "print('Shape of Representation :', representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 만들어진 Attention Score 를 values 와 내적하여,\\\n",
    "**\"Attention Score 가 반영된 Representation\"** 을 구합니다. \n",
    "\n",
    "이후, head 로 나누었던 차원을 다시 복원해 주도록 하죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Concated :  torch.Size([6, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "concated = rearrange(representation, \"b h p d -> b p (h d)\")\n",
    "print('Shape of Concated : ', concated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int = 768, \n",
    "                 num_heads: int = 8, \n",
    "                 dropout: float = 0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # keys, queries, values\n",
    "        self.K = nn.Linear(embedding_size, embedding_size)\n",
    "        self.Q = nn.Linear(embedding_size, embedding_size)\n",
    "        self.V = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "        # drop out\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.projection = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "    def forward(self, x : Tensor) -> Tensor:\n",
    "        # keys, queries, values\n",
    "        keys    = rearrange(self.K(embedded_tensor), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        queries = rearrange(self.Q(embedded_tensor), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values  = rearrange(self.V(embedded_tensor), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "\n",
    "        # Attention Score\n",
    "        QK = torch.einsum('b h q d, b h k d -> b h q k', queries, keys)\n",
    "        scaling = self.embedding_size ** (1/2)\n",
    "        attention_score = F.softmax(QK/scaling, dim=-1)\n",
    "        representation = torch.einsum('b h p d, b h d v -> b h p v ', attention_score, values)\n",
    "\n",
    "        # Concat and projection\n",
    "        concated = rearrange(representation, \"b h p d -> b p (h d)\")\n",
    "        out = self.projection(concated)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 37, 768]         590,592\n",
      "            Linear-2              [-1, 37, 768]         590,592\n",
      "            Linear-3              [-1, 37, 768]         590,592\n",
      "            Linear-4              [-1, 37, 768]         590,592\n",
      "================================================================\n",
      "Total params: 2,362,368\n",
      "Trainable params: 2,362,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.87\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 9.99\n",
      "----------------------------------------------------------------\n",
      "Output shape: torch.Size([6, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "# 지난 장\n",
    "image_embedding = Image_Embedding(image_size = ims.shape, patch_size=16).to(device)\n",
    "embedded_tensor = image_embedding(ims.to(device))\n",
    "\n",
    "\n",
    "mha = MultiHeadAttention(embedded_tensor.shape[-1], 8, 0).to(device)\n",
    "mha_pass = mha(embedded_tensor)\n",
    "summary(mha, embedded_tensor.shape[1:])\n",
    "print('Output shape: {}'.format(mha_pass.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# 정리 \n",
    "\n",
    "여기까지가 Multi-Head Attention 을 구현한 것 입니다. \n",
    "\n",
    "이제 다음장에서는, fig 1 에 나와있는 나머지 부분을 구현해 보도록 하죠!\n",
    "\n",
    "Residual connection 과, feed forward, add norm 이 남아 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
