{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "from utils.vit_utils import Image_Embedding as IE# 이전 장의 image embedding\n",
    "from utils.vit_utils import MultiHeadAttention as MHA # 이전 장의 Multi-Head Attention\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        temp_x = x\n",
    "        x = self.layer(x, **kwargs)\n",
    "        return x + temp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int,\n",
    "                 expansion: int = 4, \n",
    "                 dropout: float = 0.):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.ff_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, expansion * embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(expansion * embedding_size, embedding_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ff_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int = 768,\n",
    "                 dropout: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_dropout: float = 0,\n",
    "                 **kwargs):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm_mha = nn.Sequential(\n",
    "            ResidualConnection(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(embedding_size),\n",
    "                    MHA(embedding_size, **kwargs),\n",
    "                    nn.Dropout(dropout)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.norm_ff = nn.Sequential(\n",
    "            ResidualConnection(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(embedding_size),\n",
    "                    FeedForward(embedding_size, forward_expansion, forward_dropout),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        def forward(x):\n",
    "            x = self.norm_mha(x)\n",
    "            return self.norm_ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.multi_encoder_layer = nn.Sequential(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
