{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder\n",
    "\n",
    "지금까지 구현한 내용을 살펴보기 위해 전체 모델의 흐름을 다시 한 번 확인해 보죠.\n",
    "\n",
    "- Image -> \\[ Image_Embedding \\] -> embedded tensor\n",
    "- embedded tensor -> \\[ Transformer Encoder \\] -> extracted features\n",
    "- extracted features -> \\[ MLP Head \\] -> Class probability\n",
    "\n",
    "\n",
    "\\[ MLP Head \\] 에 대한 내용은 다음 장에서 다루도록 하고,\n",
    "\n",
    "\\[ Transformer Encoder \\] 의 구현을 위한 구성을 fig 1 과 함께 sudocode 로 확인해 보죠.\n",
    "\n",
    "<center>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./img/TransformerEncoder.png\" alt=\"Transformer Encoder\" width=\"20%\" height=\"20%\">\n",
    "</figure>\n",
    "\n",
    "<figcaption style=\"text-align:center; font-size:15px; color:#808080; margin-top:40px\">\n",
    "    \"fig 1: Transformer Encoder\"\n",
    "</figcaption>\n",
    "  \n",
    "</center>\n",
    "\n",
    "\n",
    ">```json\n",
    ">\"Transformer Encoder\":\n",
    ">{\n",
    ">  \"Transformer Block\": {\n",
    ">      Multi-Head Attention\n",
    ">      Layer Norm\n",
    ">      Residual Connections\n",
    ">      FeedForward Network\n",
    ">  } \"L times\"\n",
    ">}\n",
    ">```\n",
    "\n",
    "\\[ Transformer Encoder \\] 는 \\[ Transformer Block \\] N 개로 쌓여 있는 구조입니다. \n",
    "\n",
    "\\[ Transformer Block \\] 은 Multi-Head Attention, Layer Norm, \\\n",
    "Residual Connections, FeedForward Network 로 구성되어 있는것을 확인할 수 있습니다. \n",
    "\n",
    "이전 장에서는 \\[ Transformer Block \\] 에 요소인 Multi-Head Attention 을 구성해 보았습니다. \n",
    "\n",
    "이번 장에서는 \\[ Transformer Block \\] 에 필요한 모든 요소를 구성하여 \\[ Transformer Encoder \\] 를 완성하는 것을 목표로 합시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "from utils.vit_utils import Image_Embedding # 이전 장의 image embedding\n",
    "from utils.vit_utils import Multi_Head_Attention # 이전 장의 Multi-Head Attention\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([6, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ims = torch.Tensor(np.load('./resources/test_images.npy', allow_pickle=False))\n",
    "ims = rearrange(ims, 'b h w c -> b c h w')\n",
    "print(type(ims), ims.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## Residual Connection\n",
    "가장 먼저, Residual Connection 먼저 구성해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        temp_x = x\n",
    "        x = self.layer(x)\n",
    "        return x + temp_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig 1 에서 표현 되듯, MHA 와 MLP(FeedForward Network) 에 각각 적용될 수 있도록\\\n",
    "`__init__()` 함수에서 layer 를 인수로 받아 구성한 모습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## FeedForward Network (MLP)\n",
    "\n",
    "다음으로 FeedForward Network 를 구성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int,\n",
    "                 expansion: int = 4, \n",
    "                 dropout: float = 0.):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.ff_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, expansion * embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(expansion * embedding_size, embedding_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ff_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력으로 들어온 `embedding_size` 를 기준으로, \\\n",
    "expansion 배수만큼 parameter 를 키웠다가 복원하는 모습입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size: int = 768,\n",
    "                 dropout: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_dropout: float = 0,\n",
    "                 **kwargs):\n",
    "        super(Transformer_Block, self).__init__()\n",
    "        self.norm_mha = nn.Sequential(\n",
    "            ResidualConnection(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(embedding_size),\n",
    "                    Multi_Head_Attention(embedding_size, **kwargs),\n",
    "                    nn.Dropout(dropout)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.norm_ff = nn.Sequential(\n",
    "            ResidualConnection(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(embedding_size),\n",
    "                    FeedForward(embedding_size, forward_expansion, forward_dropout),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_mha(x)\n",
    "        return self.norm_ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.multi_encoder_layer = nn.Sequential(*[Transformer_Block(**kwargs) for _ in range(depth)])    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.multi_encoder_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1              [-1, 37, 768]           1,536\n",
      "            Linear-2              [-1, 37, 768]         590,592\n",
      "            Linear-3              [-1, 37, 768]         590,592\n",
      "            Linear-4              [-1, 37, 768]         590,592\n",
      "            Linear-5              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-6              [-1, 37, 768]               0\n",
      "           Dropout-7              [-1, 37, 768]               0\n",
      "ResidualConnection-8              [-1, 37, 768]               0\n",
      "         LayerNorm-9              [-1, 37, 768]           1,536\n",
      "           Linear-10             [-1, 37, 3072]       2,362,368\n",
      "             GELU-11             [-1, 37, 3072]               0\n",
      "          Dropout-12             [-1, 37, 3072]               0\n",
      "           Linear-13              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-14              [-1, 37, 768]               0\n",
      "          Dropout-15              [-1, 37, 768]               0\n",
      "ResidualConnection-16              [-1, 37, 768]               0\n",
      "Transformer_Block-17              [-1, 37, 768]               0\n",
      "        LayerNorm-18              [-1, 37, 768]           1,536\n",
      "           Linear-19              [-1, 37, 768]         590,592\n",
      "           Linear-20              [-1, 37, 768]         590,592\n",
      "           Linear-21              [-1, 37, 768]         590,592\n",
      "           Linear-22              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-23              [-1, 37, 768]               0\n",
      "          Dropout-24              [-1, 37, 768]               0\n",
      "ResidualConnection-25              [-1, 37, 768]               0\n",
      "        LayerNorm-26              [-1, 37, 768]           1,536\n",
      "           Linear-27             [-1, 37, 3072]       2,362,368\n",
      "             GELU-28             [-1, 37, 3072]               0\n",
      "          Dropout-29             [-1, 37, 3072]               0\n",
      "           Linear-30              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-31              [-1, 37, 768]               0\n",
      "          Dropout-32              [-1, 37, 768]               0\n",
      "ResidualConnection-33              [-1, 37, 768]               0\n",
      "Transformer_Block-34              [-1, 37, 768]               0\n",
      "        LayerNorm-35              [-1, 37, 768]           1,536\n",
      "           Linear-36              [-1, 37, 768]         590,592\n",
      "           Linear-37              [-1, 37, 768]         590,592\n",
      "           Linear-38              [-1, 37, 768]         590,592\n",
      "           Linear-39              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-40              [-1, 37, 768]               0\n",
      "          Dropout-41              [-1, 37, 768]               0\n",
      "ResidualConnection-42              [-1, 37, 768]               0\n",
      "        LayerNorm-43              [-1, 37, 768]           1,536\n",
      "           Linear-44             [-1, 37, 3072]       2,362,368\n",
      "             GELU-45             [-1, 37, 3072]               0\n",
      "          Dropout-46             [-1, 37, 3072]               0\n",
      "           Linear-47              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-48              [-1, 37, 768]               0\n",
      "          Dropout-49              [-1, 37, 768]               0\n",
      "ResidualConnection-50              [-1, 37, 768]               0\n",
      "Transformer_Block-51              [-1, 37, 768]               0\n",
      "        LayerNorm-52              [-1, 37, 768]           1,536\n",
      "           Linear-53              [-1, 37, 768]         590,592\n",
      "           Linear-54              [-1, 37, 768]         590,592\n",
      "           Linear-55              [-1, 37, 768]         590,592\n",
      "           Linear-56              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-57              [-1, 37, 768]               0\n",
      "          Dropout-58              [-1, 37, 768]               0\n",
      "ResidualConnection-59              [-1, 37, 768]               0\n",
      "        LayerNorm-60              [-1, 37, 768]           1,536\n",
      "           Linear-61             [-1, 37, 3072]       2,362,368\n",
      "             GELU-62             [-1, 37, 3072]               0\n",
      "          Dropout-63             [-1, 37, 3072]               0\n",
      "           Linear-64              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-65              [-1, 37, 768]               0\n",
      "          Dropout-66              [-1, 37, 768]               0\n",
      "ResidualConnection-67              [-1, 37, 768]               0\n",
      "Transformer_Block-68              [-1, 37, 768]               0\n",
      "        LayerNorm-69              [-1, 37, 768]           1,536\n",
      "           Linear-70              [-1, 37, 768]         590,592\n",
      "           Linear-71              [-1, 37, 768]         590,592\n",
      "           Linear-72              [-1, 37, 768]         590,592\n",
      "           Linear-73              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-74              [-1, 37, 768]               0\n",
      "          Dropout-75              [-1, 37, 768]               0\n",
      "ResidualConnection-76              [-1, 37, 768]               0\n",
      "        LayerNorm-77              [-1, 37, 768]           1,536\n",
      "           Linear-78             [-1, 37, 3072]       2,362,368\n",
      "             GELU-79             [-1, 37, 3072]               0\n",
      "          Dropout-80             [-1, 37, 3072]               0\n",
      "           Linear-81              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-82              [-1, 37, 768]               0\n",
      "          Dropout-83              [-1, 37, 768]               0\n",
      "ResidualConnection-84              [-1, 37, 768]               0\n",
      "Transformer_Block-85              [-1, 37, 768]               0\n",
      "        LayerNorm-86              [-1, 37, 768]           1,536\n",
      "           Linear-87              [-1, 37, 768]         590,592\n",
      "           Linear-88              [-1, 37, 768]         590,592\n",
      "           Linear-89              [-1, 37, 768]         590,592\n",
      "           Linear-90              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-91              [-1, 37, 768]               0\n",
      "          Dropout-92              [-1, 37, 768]               0\n",
      "ResidualConnection-93              [-1, 37, 768]               0\n",
      "        LayerNorm-94              [-1, 37, 768]           1,536\n",
      "           Linear-95             [-1, 37, 3072]       2,362,368\n",
      "             GELU-96             [-1, 37, 3072]               0\n",
      "          Dropout-97             [-1, 37, 3072]               0\n",
      "           Linear-98              [-1, 37, 768]       2,360,064\n",
      "      FeedForward-99              [-1, 37, 768]               0\n",
      "         Dropout-100              [-1, 37, 768]               0\n",
      "ResidualConnection-101              [-1, 37, 768]               0\n",
      "Transformer_Block-102              [-1, 37, 768]               0\n",
      "       LayerNorm-103              [-1, 37, 768]           1,536\n",
      "          Linear-104              [-1, 37, 768]         590,592\n",
      "          Linear-105              [-1, 37, 768]         590,592\n",
      "          Linear-106              [-1, 37, 768]         590,592\n",
      "          Linear-107              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-108              [-1, 37, 768]               0\n",
      "         Dropout-109              [-1, 37, 768]               0\n",
      "ResidualConnection-110              [-1, 37, 768]               0\n",
      "       LayerNorm-111              [-1, 37, 768]           1,536\n",
      "          Linear-112             [-1, 37, 3072]       2,362,368\n",
      "            GELU-113             [-1, 37, 3072]               0\n",
      "         Dropout-114             [-1, 37, 3072]               0\n",
      "          Linear-115              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-116              [-1, 37, 768]               0\n",
      "         Dropout-117              [-1, 37, 768]               0\n",
      "ResidualConnection-118              [-1, 37, 768]               0\n",
      "Transformer_Block-119              [-1, 37, 768]               0\n",
      "       LayerNorm-120              [-1, 37, 768]           1,536\n",
      "          Linear-121              [-1, 37, 768]         590,592\n",
      "          Linear-122              [-1, 37, 768]         590,592\n",
      "          Linear-123              [-1, 37, 768]         590,592\n",
      "          Linear-124              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-125              [-1, 37, 768]               0\n",
      "         Dropout-126              [-1, 37, 768]               0\n",
      "ResidualConnection-127              [-1, 37, 768]               0\n",
      "       LayerNorm-128              [-1, 37, 768]           1,536\n",
      "          Linear-129             [-1, 37, 3072]       2,362,368\n",
      "            GELU-130             [-1, 37, 3072]               0\n",
      "         Dropout-131             [-1, 37, 3072]               0\n",
      "          Linear-132              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-133              [-1, 37, 768]               0\n",
      "         Dropout-134              [-1, 37, 768]               0\n",
      "ResidualConnection-135              [-1, 37, 768]               0\n",
      "Transformer_Block-136              [-1, 37, 768]               0\n",
      "       LayerNorm-137              [-1, 37, 768]           1,536\n",
      "          Linear-138              [-1, 37, 768]         590,592\n",
      "          Linear-139              [-1, 37, 768]         590,592\n",
      "          Linear-140              [-1, 37, 768]         590,592\n",
      "          Linear-141              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-142              [-1, 37, 768]               0\n",
      "         Dropout-143              [-1, 37, 768]               0\n",
      "ResidualConnection-144              [-1, 37, 768]               0\n",
      "       LayerNorm-145              [-1, 37, 768]           1,536\n",
      "          Linear-146             [-1, 37, 3072]       2,362,368\n",
      "            GELU-147             [-1, 37, 3072]               0\n",
      "         Dropout-148             [-1, 37, 3072]               0\n",
      "          Linear-149              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-150              [-1, 37, 768]               0\n",
      "         Dropout-151              [-1, 37, 768]               0\n",
      "ResidualConnection-152              [-1, 37, 768]               0\n",
      "Transformer_Block-153              [-1, 37, 768]               0\n",
      "       LayerNorm-154              [-1, 37, 768]           1,536\n",
      "          Linear-155              [-1, 37, 768]         590,592\n",
      "          Linear-156              [-1, 37, 768]         590,592\n",
      "          Linear-157              [-1, 37, 768]         590,592\n",
      "          Linear-158              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-159              [-1, 37, 768]               0\n",
      "         Dropout-160              [-1, 37, 768]               0\n",
      "ResidualConnection-161              [-1, 37, 768]               0\n",
      "       LayerNorm-162              [-1, 37, 768]           1,536\n",
      "          Linear-163             [-1, 37, 3072]       2,362,368\n",
      "            GELU-164             [-1, 37, 3072]               0\n",
      "         Dropout-165             [-1, 37, 3072]               0\n",
      "          Linear-166              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-167              [-1, 37, 768]               0\n",
      "         Dropout-168              [-1, 37, 768]               0\n",
      "ResidualConnection-169              [-1, 37, 768]               0\n",
      "Transformer_Block-170              [-1, 37, 768]               0\n",
      "       LayerNorm-171              [-1, 37, 768]           1,536\n",
      "          Linear-172              [-1, 37, 768]         590,592\n",
      "          Linear-173              [-1, 37, 768]         590,592\n",
      "          Linear-174              [-1, 37, 768]         590,592\n",
      "          Linear-175              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-176              [-1, 37, 768]               0\n",
      "         Dropout-177              [-1, 37, 768]               0\n",
      "ResidualConnection-178              [-1, 37, 768]               0\n",
      "       LayerNorm-179              [-1, 37, 768]           1,536\n",
      "          Linear-180             [-1, 37, 3072]       2,362,368\n",
      "            GELU-181             [-1, 37, 3072]               0\n",
      "         Dropout-182             [-1, 37, 3072]               0\n",
      "          Linear-183              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-184              [-1, 37, 768]               0\n",
      "         Dropout-185              [-1, 37, 768]               0\n",
      "ResidualConnection-186              [-1, 37, 768]               0\n",
      "Transformer_Block-187              [-1, 37, 768]               0\n",
      "       LayerNorm-188              [-1, 37, 768]           1,536\n",
      "          Linear-189              [-1, 37, 768]         590,592\n",
      "          Linear-190              [-1, 37, 768]         590,592\n",
      "          Linear-191              [-1, 37, 768]         590,592\n",
      "          Linear-192              [-1, 37, 768]         590,592\n",
      "Multi_Head_Attention-193              [-1, 37, 768]               0\n",
      "         Dropout-194              [-1, 37, 768]               0\n",
      "ResidualConnection-195              [-1, 37, 768]               0\n",
      "       LayerNorm-196              [-1, 37, 768]           1,536\n",
      "          Linear-197             [-1, 37, 3072]       2,362,368\n",
      "            GELU-198             [-1, 37, 3072]               0\n",
      "         Dropout-199             [-1, 37, 3072]               0\n",
      "          Linear-200              [-1, 37, 768]       2,360,064\n",
      "     FeedForward-201              [-1, 37, 768]               0\n",
      "         Dropout-202              [-1, 37, 768]               0\n",
      "ResidualConnection-203              [-1, 37, 768]               0\n",
      "Transformer_Block-204              [-1, 37, 768]               0\n",
      "================================================================\n",
      "Total params: 85,054,464\n",
      "Trainable params: 85,054,464\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 67.64\n",
      "Params size (MB): 324.46\n",
      "Estimated Total Size (MB): 392.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "image_embedding = Image_Embedding(image_size = ims.shape, patch_size=16).to(device)\n",
    "embedded_tensor = image_embedding(ims.to(device))\n",
    "\n",
    "Vit = TransformerEncoder(embedding_size = 768, num_heads = 8).to(device)\n",
    "summary(Vit, embedded_tensor.shape[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
